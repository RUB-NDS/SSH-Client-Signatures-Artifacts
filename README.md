# On the Security of SSH Client Signatures - Artifacts

This repository contains the artifacts for the paper "On the Security of SSH Client Signatures",
accepted at the ACM Conference on Computer and Communications Security (CCS) 2025.

## Prerequisites

### Hardware Requirements

We recommend having a machine with at least 16 CPU cores and 64 GB of RAM available.
This is required to run the Elasticsearch stack and the various analysis tools included in this repository.
Running with less than the recommended hardware may result in degraded performance or unpredictable results.

We ran our experiments on a server with 2 AMD EPYC 7763 64-core CPUs and 2 TB of RAM.

### Software Requirements

- Ubuntu 24.04 LTS. We recommend a fresh installation to avoid possible side effects.
- Docker 28.3.3 or newer.
- Golang 1.25.0 or newer.
- Python 3.11.
- SageMath 10.3 installed as a Python library.

To ease installation, we provide a setup script (`scripts/00_setup_env.sh`) that automates the installation of the required software and dependencies. It is designed to be run on a fresh Ubuntu 24.04 LTS installation. In particular, it will perform the following steps:

1. Install system packages
    - Latest version of Docker
    - Python 3.11 with venv module installed
    - SageMath 10.3 dependencies
2. Install Golang 1.25.0
3. Sets up a Python virtual environment in `venv/`
4. Install required Python dependencies for the evaluation scripts and tools into the virtual environment
5. Install SageMath 10.3 as a Python library
6. Build the key_scraper and nonce_sampler tools using `go build`
7. Set up Docker containers for the Elasticsearch infrastructure
8. Copy the auto-generated Elasticsearch CA certificate to the host (`code/key_scraper/ca.crt`)

> [!IMPORTANT]
> After running the setup script, you must restart your system to ensure all changes take effect.

> [!TIP]
> Building SageMath 10.3 as a Python library can take a significant amount of time, please be patient. You may track the progress by tailing the log file (`tail -f logs/setup_env.log`).

### Other Requirements

For evaluation, accounts on GitHub, GitLab, and Launchpad are required to access the API (GitHub and GitLab only) and to upload generated SSH keys while testing for public key upload restrictions. Free accounts are sufficient for this purpose. Uploaded SSH keys should be removed immediately after testing.

### Basic Functionality Test

To test basic functionality after running `scripts/00_setup_env.sh`, you can perform the following checks:

1. Verify that Docker, Golang, and Python are installed and accessible from the command line:
   ```bash
   docker --version
   go version
   python3.11 --version
   ```

2. Verify that the Python virtual environment can be activated and, in particular, that you can load the SageMath library:
   ```bash
   . venv/bin/activate
   python -c "from sage.all import Primes; print(Primes())"
   deactivate
   ```

3. Verify that the Elasticsearch stack is running and that both, Elasticsearch and Kibana, are available:
   ```bash
   docker ps
   curl -u elastic:elasticsearchpass --cacert code/key_scraper/ca.crt "https://localhost:9200/_cluster/health?pretty"
   curl "http://localhost:5601/api/status"
   ```

## Claims

The following major claims are made with regard to the artifacts:

1. The `key_scraper` tool (`code/key_scraper`) is capable of collecting SSH public keys from multiple platforms (GitHub, GitLab, Launchpad) in a systematic manner.
2. The evaluation pipeline (`code/key_scraper/scripts`) can identify and analyze weak SSH keys based on the methodology described in section 3.2 in the paper.
3. The `07-generate-test-keys.py` script can generate a diverse set of SSH keys violating certain properties for testing upload restrictions. At the time of artifact evaluation, these keys were able to reproduce table 2 in the paper.
4. The `nonce_sampler` tool (`code/nonce_sampler`) can test ECDSA and EdDSA signatures generated by SSH clients and agents for determinism and potential bias. It can find the biased nonce vulnerability in PuTTY <= 0.80 ([CVE-2024-31497](https://www.cve.org/CVERecord?id=CVE-2024-31497)).

> [!TIP]
> Each claim can be proven by the corresponding experiment with the same number, that is, claim 1 can be proven by E1, claim 2 by E2, and so on.

> [!NOTE]
> Our paper is based on two full runs of the key scraper evaluation pipeline (claims 1 and 2) in June 2023 and January 2025. Exact reproduction of the results in the paper is not possible due to the changing nature of the source data. Additionally, a full run will require over a month of real time due to API limits. To account for this, the experiments E1 and E2 are designed to demonstrate the capabilities of the proposed tools and methodologies at a small scale reasonable for evaluation.

## Experiments

For running the artifacts, you can use the provided scripts in the `scripts/` directory. Each script is designed to perform a specific task in the evaluation pipeline.

### E1 - Collecting SSH Public Keys

To collect SSH public keys from GitHub, GitLab and Launchpad, you can use the `scripts/01_run_scraper.sh` script. This script will configure and then start the key scraper and collect SSH public keys from the platforms for a duration of 24 hours. Adjust the timeout in the script as needed (or remove entirely).

To run the scraper on GitHub and GitLab, you will need to provide a personal access token to authenticate with the API of each service. Refer to [GitHub Docs](https://docs.github.com/en/rest/authentication/authenticating-to-the-rest-api?apiVersion=2022-11-28#authenticating-with-a-personal-access-token) and [GitLab Docs](https://docs.gitlab.com/user/profile/personal_access_tokens/) for more information on how to generate one. For GitHub, we used a classic access token rather than a fine-grained one - if you decide to generate a fine-grained token, selecting public repository access and no account permissions should be sufficient. For GitLab, you must assign at least `read_api` and `read_user` scopes to the access token.

> [!NOTE]
> Since our evaluation in January 2025, GitLab started to enforce stricter rate limits on their API, which may negatively affect the data collection process. See [their announcement](https://about.gitlab.com/blog/rate-limitations-announced-for-projects-groups-and-users-apis/) for further details. Our testing indicates that these rate limits only slow down, but not prevent, the data collection process.

After execution, the Elasticsearch database should be populated with user records from all three platforms containing their SSH public keys. You can use Kibana (available at `http://localhost:5601`) to explore the collected dataset. Use `elastic:elasticsearchpass` as the credentials to log in. Navigate to the "Discover" app (`http://localhost:5601/app/discover`) and create a new data view (see [Elastic Docs](https://www.elastic.co/docs/explore-analyze/find-and-organize/data-views)) with an `sshks_users*` index pattern. For the timestamp field, select `visitedAt`.

### E2 - Running the Evaluation Pipeline

Once you have collected a decent amount of SSH public keys, run the evaluation by pipeline by calling `scripts/02_evaluate_keys.sh`. This script will perform a full run of the evaluation pipeline on the keys collected by the key scraper tool. In particular, it will perform the following steps:

0. Activate the virtual environment in `venv`
1. Extract the SSH keys from the user-oriented scraping results (`01-extract-keys-sshks.py`). Errors are logged to `results/01-extract-keys-sshks-errors.txt`
2. Collect unique SSH keys and stores it into a separate index (`01-extract-keys-sshks-unique.py`).
3. Evaluate basic statistics for the dataset (`02-summarize-keys.py`):
    - `results/02-summary-ecdsa-curve-dist.png` - Distribution of ECDSA key curves used in the collected SSH keys (figure 4 in the paper).
    - `results/02-summary-rsa-modulus-cdf.png` - (Complementary) CDF of RSA key modulus sizes used in the collected SSH keys.
    - `results/02-summary-rsa-modulus-dist.png` - Distribution of RSA key modulus sizes used in the collected SSH keys (figure 3 in the paper).
    - `02-summary.tex` - LaTeX table used as a base for table 1 in the paper.
4. Analyze the SSH keys according to the methodology described in section 3.2 of the paper.
5. Generate a report summarizing the findings of the evaluation pipeline.
6. Collect users affected by weak keys from the Elasticsearch database for disclosure.

All results can be found in the `results` directory after running the script. Additionally, the summarized findings are printed to stdout.

### E3 - Testing Public Key Upload Restrictions

To test the public key upload restrictions of various platforms, you can generate SSH keys with specific properties (e.g., weak keys) and attempt to upload them to the platforms. To generate the keys, run `scripts/03_generate_test_keys.sh`. This will call the `07-generate-test-keys.py` python script and store the generated keys in the `results` directory. The resulting file contains one SSH key per line that can be copied into the corresponding SSH key upload forms on [GitHub](https://github.com/settings/ssh/new), [GitLab](https://gitlab.com/-/user_settings/ssh_keys), and Launchpad (https://launchpad.net/~*user*/+editsshkeys). If the key is accepted without error and is visible in the account's list of SSH keys, we consider the upload successful. Uploading each key to each platform can reproduce table 2 in the paper.

### E4 - Testing SSH Clients and Agents for Nonce Determinism and Bias

TODO

## Repository Structure

```text
.
├── code
│   ├── env_docker                 # Dockerfiles for creating the Elasticsearch infrastructure used for evalutaion
│   ├── key_scraper                # A tool written in Go that can collect SSH public keys from GitHub, GitLab, and Launchpad
|   |   └── scripts                # Scripts for evaluating a dataset of SSH public keys gathered with the key_scraper tool
│   ├── nonce_sampler              # A tool written in Go that can be used to analyze the determinism and bias of SSH client nonces
│   └── rsa_factorability_tool     # Python implementation for an optimized batch-gcd algorithm used to find common factors
├── data
│   ├── key_scraper_results        # Experimental results of the key_scraper used in the paper (excluding raw data)
│   └── putty_attack               # Experimental results with regard to the PuTTY vulnerability
├── scripts
│   ├── 00_setup_env.sh            # Script for setting up a fresh evaluation environment with all dependencies installed
│   ├── 01_run_scraper.sh          # Runs the key_scraper tool on GitHub, Gitlab, and Launchpad for 24 hours
│   └── 02_evaluate_keys.sh        # Performs a full run of the evaluation pipeline on the keys collected by the key_scraper tool
└── README.md
```

## Acknowledgements

We use a variety of third party libaries and tools for the tools and scripts contained in this repository.

### Key Scraper (`code/key_scraper`)

- GraphQL Client - [github.com/Khan/genqlient](https://github.com/Khan/genqlient)
- Golang Elasticsearch Library - [github.com/elastic/go-elasticsearch](https://github.com/elastic/go-elasticsearch)
- Scheduling Library - [github.com/reugn/go-quartz](https://github.com/reugn/go-quartz)
- Configuration Library - [github.com/spf13/viper](https://github.com/spf13/viper)
- GraphQL Parser - [github.com/vektah/gqlparser](https://github.com/vektah/gqlparser)

### Key Scraper Evaluation Scripts (`code/key_scraper/scripts`)

- badkeys Tool - [badkeys](https://pypi.org/project/badkeys/)
- Cryptographic Library - [cryptography](https://pypi.org/project/cryptography/)
- Elliptic Curve Library - [ECPy](https://pypi.org/project/ECPy/)
- Python Elasticsearch Library - [elasticsearch](https://pypi.org/project/elasticsearch/)
- Multi-precision Arithmetic Library - [gmpy2](https://pypi.org/project/gmpy2/)
- Plotting Library - [matplotlib](https://pypi.org/project/matplotlib/)
- Multiprocessing Library - [mpire](https://pypi.org/project/mpire/)
- NumPy - [numpy](https://pypi.org/project/numpy/)
- ROCA Detection Library - [roca-detect](https://pypi.org/project/roca-detect/)
- Progress Bar - [tqdm](https://pypi.org/project/tqdm/)

### RSA Factorability Tool (`code/rsa_factorability_tool`)

- badkeys Tool - [badkeys](https://pypi.org/project/badkeys/)
- Cryptographic Library - [cryptography](https://pypi.org/project/cryptography/)
- WSGI Web Application Framework - [Flask](https://pypi.org/project/Flask/)
- Multi-precision Arithmetic Library - [gmpy2](https://pypi.org/project/gmpy2/)
- Python MongoDB Library - [pymongo](https://pypi.org/project/pymongo/)
- WSGI Server - [waitress](https://pypi.org/project/waitress/)
- Integer Factorization - [primefac](https://pypi.org/project/primefac/)
- Progress Bar - [tqdm](https://pypi.org/project/tqdm/)
- Python Elasticsearch Library - [elasticsearch](https://pypi.org/project/elasticsearch/)

### Client Nonce Sampler (`code/nonce_sampler`)

- Implementation of the Edwards25519 Curve - [filippo.io/edwards25519](https://filippo.io/edwards25519)
- Colorized Output - [github.com/fatih/color](https://github.com/fatih/color)
- Tabular Output - [github.com/rodaine/table](https://github.com/rodaine/table)
- CLI Interface - [github.com/urfave/cli](https://github.com/urfave/cli)
- Queue Implementation - [go.linecorp.com/garr](https://go.linecorp.com/garr)
- Cryptographic Library / SSH Implementation - [golang.org/x/crypto](https://golang.org/x/crypto) (modified)
