#!/usr/bin/env python3
#
# Usage: ./01-extract-keys-sshks-unique.py
#
# This script collects the SSH public keys from the separate key indices
# generated by ./01-extract-keys-sshks.py, which are organized by user, and
# puts them into a combined index. Each distinct SSH public key is only saved
# once, with references to the users who own them.
#
from elasticsearch import Elasticsearch
from elasticsearch.helpers import parallel_bulk, scan
from tqdm import tqdm

from config import *

SRC_INDICES = [INDEX_KEYS_GITHUB, INDEX_KEYS_GITLAB, INDEX_KEYS_LAUNCHPAD]
DEST_INDEX = INDEX_KEYS_UNIQUE
MAPPINGS = {
    "properties": {
        "entry": {"type": "integer"},
        "alg": {"type": "keyword"},
        "outer_alg": {"type": "keyword"},
        "length": {"type": "integer"},
        "source": {
            "type": "object",
            "properties": {"index": {"type": "keyword"}, "id": {"type": "keyword"}},
        },
        "fpr": {"type": "keyword"},
        "params": {
            "type": "object",
            "properties": {
                "e": {"type": "text"},
                "n": {"type": "text"},
                "p": {"type": "text"},
                "q": {"type": "text"},
                "g": {"type": "text"},
                "y": {"type": "text"},
                "curve": {"type": "keyword"},
                "Q": {"type": "text"},
                "key": {"type": "text"},
            },
        },
    }
}


class KeyIterator(object):
    def __init__(self, batchsize=10000, scroll="60m", bulk_threads=16):
        self.batchsize = batchsize
        self.scroll = scroll
        self.bulk_threads = bulk_threads

    def __enter__(self):
        # Connect to Elasticsearch
        self.es = Elasticsearch(
            ES_URL,
            ca_certs=ES_CA_CERT,
            basic_auth=(ES_USER, ES_PASSWORD),
            request_timeout=ES_REQUEST_TIMEOUT
        )
        if not self.es.ping():
            tqdm.write("Could not reach Elasticsearch. Abort.")
            raise ConnectionError("Could not reach Elasticsearch.")
        for index in SRC_INDICES:
            if not self.es.indices.exists(index=index):
                self.es.indices.create(index=index)
        # Adjust result window on source indices to allow for larger batch sizes.
        self.es.indices.put_settings(
            index=SRC_INDICES,
            body={"index.max_result_window": max(self.batchsize, 10000)},
        )
        # Drop the index if it already exists.
        if self.es.indices.exists(index=DEST_INDEX):
            self.es.indices.delete(index=DEST_INDEX)
            self.es.indices.create(
                index=DEST_INDEX,
                mappings=MAPPINGS,
                settings={
                    "number_of_shards": NUM_SHARDS,
                    "number_of_replicas": NUM_REPLICAS
                },
            )
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # Close elastic connection and progress bars.
        self.es.close()

    def get_total_key_count(self):
        return self.es.count(index=SRC_INDICES)["count"]

    def run(self):
        # Query to retrieve all keys from the source indices.
        query = {"query": {"match_all": {}}}
        key_cnt = self.get_total_key_count()

        # Retrieve all keys from the source indices and map
        # them to new indexing actions for the destination index.
        actions = [
            {
                "_op_type": "index",
                "_index": DEST_INDEX,
                "_source": hit["_source"],
            }
            for hit in tqdm(
                scan(
                    self.es,
                    index=SRC_INDICES,
                    scroll=self.scroll,
                    query=query,
                    size=self.batchsize,
                    request_timeout=ES_REQUEST_TIMEOUT,
                ),
                desc="Keys retrieved",
                total=key_cnt,
                unit="keys",
                position=0,
            )
        ]

        # Merge actions with the same key fingerprint.
        entry = 0
        fpr_to_action = dict()
        for action in tqdm(
            actions, desc="Duplicate keys merged", unit="keys", position=1
        ):
            fpr = action["_source"]["fpr"]
            # If the fpr is already in the list, append the source, effectively merging the two records.
            if fpr in fpr_to_action:
                fpr_to_action[fpr]["_source"]["source"].extend(
                    action["_source"]["source"]
                )
            else:
                fpr_to_action[fpr] = action
                fpr_to_action[fpr]["_source"]["entry"] = entry
                entry += 1
        actions = list(fpr_to_action.values())

        with tqdm(
            total=len(actions),
            position=2,
            desc="Unique keys indexed",
            unit="keys",
        ) as indexbar:
            # Wrap the actions list in a generator that increments the indexbar.
            def actions_with_indexbar():
                for action in actions:
                    indexbar.update(1)
                    yield action

            # Create generator and bulk insert into the database.
            actions_generator = actions_with_indexbar()
            failures = 0
            for success, info in parallel_bulk(
                self.es,
                actions_generator,
                thread_count=self.bulk_threads,
                chunk_size=1000,
                queue_size=self.bulk_threads * 2):
                if not success:
                    failures += 1
                    tqdm.write(f"A document failed insertion into destination index: {info}")
            tqdm.write(
                f"Indexed {len(actions) - failures} keys of " + str(len(actions)) + " keys"
            )


if __name__ == "__main__":
    with KeyIterator(batchsize=BATCH_SIZE) as iterator:
        iterator.run()
